#!/usr/bin/env ts-node

/**
 * End-to-End Real Data Flow Test
 * 
 * This script tests the complete data flow from Pushshift archives through
 * the entire processing pipeline as specified in the PRD:
 * 
 * Reddit Archives ‚Üí Stream Processing ‚Üí LLM Analysis ‚Üí Entity Resolution ‚Üí 
 * Unified Processing ‚Üí Database Storage ‚Üí Quality Scoring
 * 
 * Tests REAL data, not mocks. Uses limited data volumes for testing.
 */

import { NestFactory } from '@nestjs/core';
import { AppModule } from './src/app.module';
import { StreamProcessorService } from './src/modules/content-processing/reddit-collector/stream-processor.service';
import { UnifiedProcessingService } from './src/modules/content-processing/reddit-collector/unified-processing.service';
import { RedditService } from './src/modules/external-integrations/reddit/reddit.service';
import { LLMService } from './src/modules/external-integrations/llm/llm.service';
import { EntityRepository } from './src/repositories/entity.repository';
import { ConnectionRepository } from './src/repositories/connection.repository';
import { LoggerService } from './src/shared';
import { RedditDataExtractorService } from './src/modules/content-processing/reddit-collector/reddit-data-extractor.service';
import { BatchProcessingCoordinatorService } from './src/modules/content-processing/reddit-collector/batch-processing-coordinator.service';
import { DataSourceType } from './src/modules/content-processing/reddit-collector/data-merge.types';

interface TestResults {
  archiveProcessing: {
    filesProcessed: number;
    recordsExtracted: number;
    processingTime: number;
  };
  llmProcessing: {
    contentAnalyzed: number;
    entitiesExtracted: number;
    processingTime: number;
  };
  entityResolution: {
    entitiesResolved: number;
    entitiesCreated: number;
    processingTime: number;
  };
  databaseOperations: {
    connectionsCreated: number;
    mentionsCreated: number;
    processingTime: number;
  };
  qualityScoring: {
    scoresComputed: number;
    processingTime: number;
  };
  redditApiTest: {
    postsRetrieved: number;
    commentsRetrieved: number;
    processingTime: number;
  };
}

async function runEndToEndTest(): Promise<TestResults> {
  console.log('üöÄ Starting End-to-End Real Data Flow Test');
  console.log('=====================================');
  
  const app = await NestFactory.createApplicationContext(AppModule);
  await app.init();
  
  // Get all required services
  const streamProcessor = app.get(StreamProcessorService);
  const redditExtractor = app.get(RedditDataExtractorService);
  const batchCoordinator = app.get(BatchProcessingCoordinatorService);
  const unifiedProcessing = app.get(UnifiedProcessingService);
  const redditService = app.get(RedditService);
  const llmService = app.get(LLMService);
  const entityRepo = app.get(EntityRepository);
  const connectionRepo = app.get(ConnectionRepository);
  const logger = app.get(LoggerService).setContext('E2ETest');
  
  const results: TestResults = {
    archiveProcessing: { filesProcessed: 0, recordsExtracted: 0, processingTime: 0 },
    llmProcessing: { contentAnalyzed: 0, entitiesExtracted: 0, processingTime: 0 },
    entityResolution: { entitiesResolved: 0, entitiesCreated: 0, processingTime: 0 },
    databaseOperations: { connectionsCreated: 0, mentionsCreated: 0, processingTime: 0 },
    qualityScoring: { scoresComputed: 0, processingTime: 0 },
    redditApiTest: { postsRetrieved: 0, commentsRetrieved: 0, processingTime: 0 }
  };

  try {
    // Test 1: Archive Processing (Pushshift data)
    console.log('\nüìÅ STEP 1: Testing Pushshift Archive Processing');
    console.log('-----------------------------------------------');
    
    const archiveStartTime = Date.now();
    
    // Process small sample from austinfood archives 
    const archivePath = '/Users/brandonkimble/crave-search/apps/api/data/pushshift/archives/austinfood/austinfood_submissions.zst';
    
    console.log(`Processing archive: ${archivePath}`);
    
    // Use batch coordinator to process with small limits for testing
    const batchConfig = {
      baseBatchSize: 50, // Small batch for testing
      maxMemoryUsage: 100 * 1024 * 1024, // 100MB limit for testing
      enableCheckpoints: false, // Disable for simple test
      progressReportingInterval: 10, // Report every 10 records
    };
    
    // Process the archive with limited data
    const streamResult = await batchCoordinator.processArchiveFile(
      archivePath,
      batchConfig
    );
    
    results.archiveProcessing.filesProcessed = 1;
    results.archiveProcessing.recordsExtracted = streamResult.metrics.validItems || 0;
    results.archiveProcessing.processingTime = streamResult.metrics.duration || (Date.now() - archiveStartTime);
    
    console.log(`‚úÖ Archive processing completed:`);
    console.log(`   Records extracted: ${results.archiveProcessing.recordsExtracted}`);
    console.log(`   Processing time: ${results.archiveProcessing.processingTime}ms`);
    
    // Test 2: Reddit API Data Collection
    console.log('\nüåê STEP 2: Testing Reddit API Collection');
    console.log('---------------------------------------');
    
    const apiStartTime = Date.now();
    
    try {
      // Test Reddit API with small data sample
      const subreddit = 'austinfood';
      const limit = 10; // Small limit for testing
      
      console.log(`Fetching recent posts from r/${subreddit} (limit: ${limit})`);
      
      const redditData = await redditService.getChronologicalPosts(subreddit, limit);
      
      results.redditApiTest.postsRetrieved = redditData.data.length;
      results.redditApiTest.commentsRetrieved = 0; // CollectionMethodResult doesn't separate comments
      results.redditApiTest.processingTime = Date.now() - apiStartTime;
      
      console.log(`‚úÖ Reddit API collection completed:`);
      console.log(`   Posts retrieved: ${results.redditApiTest.postsRetrieved}`);
      console.log(`   Comments retrieved: ${results.redditApiTest.commentsRetrieved}`);
      console.log(`   API processing time: ${results.redditApiTest.processingTime}ms`);
      
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      console.log(`‚ö†Ô∏è  Reddit API test failed: ${errorMessage}`);
      console.log('   Continuing with archive data only...');
    }
    
    // Test 3: LLM Processing (if we have data from previous steps)
    console.log('\nüß† STEP 3: Testing LLM Content Analysis');
    console.log('-------------------------------------');
    
    const llmStartTime = Date.now();
    
    // Get sample data for LLM processing (either from archives or API)
    let sampleData;
    if (streamResult.extractedData && streamResult.extractedData.length > 0) {
      // Use archive data
      sampleData = streamResult.extractedData.slice(0, 5); // Limit to 5 posts for testing
      console.log(`Using ${sampleData.length} posts from archive data for LLM processing`);
    } else {
      console.log('‚ö†Ô∏è  No archive data available, skipping LLM processing test');
      sampleData = [];
    }
    
    if (sampleData.length > 0) {
      try {
        // Process through LLM service
        const llmInput = {
          posts: sampleData.map((post: any) => ({
            post_id: post.id,
            title: post.title || '',
            content: post.selftext || post.body || '',
            subreddit: post.subreddit,
            url: `https://reddit.com${post.permalink}`,
            upvotes: post.score || 0,
            created_at: new Date(post.created_utc * 1000),
            comments: []
          }))
        };
        
        const llmResult = await llmService.processContent(llmInput);
        
        results.llmProcessing.contentAnalyzed = sampleData.length;
        results.llmProcessing.entitiesExtracted = llmResult.mentions?.length || 0;
        results.llmProcessing.processingTime = Date.now() - llmStartTime;
        
        console.log(`‚úÖ LLM processing completed:`);
        console.log(`   Content pieces analyzed: ${results.llmProcessing.contentAnalyzed}`);
        console.log(`   Entities extracted: ${results.llmProcessing.entitiesExtracted}`);
        console.log(`   LLM processing time: ${results.llmProcessing.processingTime}ms`);
        
        // Test 4: Unified Processing Pipeline
        if (results.llmProcessing.entitiesExtracted > 0) {
          console.log('\n‚öôÔ∏è  STEP 4: Testing Unified Processing Pipeline');
          console.log('--------------------------------------------');
          
          const unifiedStartTime = Date.now();
          
          // Create unified processing input using proper MergedLLMInputDto structure
          const unifiedInput = {
            posts: llmInput.posts,
            comments: [], // No comments in this test
            sourceMetadata: {
              batchId: `test_${Date.now()}`,
              mergeTimestamp: new Date(),
              sourceBreakdown: {
                [DataSourceType.PUSHSHIFT_ARCHIVE]: sampleData.length,
                [DataSourceType.REDDIT_API_CHRONOLOGICAL]: 0,
                [DataSourceType.REDDIT_API_KEYWORD_SEARCH]: 0,
                [DataSourceType.REDDIT_API_ON_DEMAND]: 0
              },
              temporalRange: {
                earliest: Math.min(...sampleData.map(p => p.created_utc)),
                latest: Math.max(...sampleData.map(p => p.created_utc)),
                spanHours: 24
              }
            }
          };
          
          try {
            const unifiedResult = await unifiedProcessing.processUnifiedBatch(unifiedInput);
            
            results.entityResolution.entitiesResolved = unifiedResult.entityResolution.entitiesProcessed || 0;
            results.databaseOperations.connectionsCreated = unifiedResult.databaseOperations.connectionsCreated || 0;
            results.databaseOperations.mentionsCreated = unifiedResult.databaseOperations.mentionsCreated || 0;
            results.databaseOperations.processingTime = Date.now() - unifiedStartTime;
            
            console.log(`‚úÖ Unified processing completed:`);
            console.log(`   Entities processed: ${results.entityResolution.entitiesResolved}`);
            console.log(`   Connections created: ${results.databaseOperations.connectionsCreated}`);
            console.log(`   Mentions created: ${results.databaseOperations.mentionsCreated}`);
            console.log(`   Pipeline processing time: ${results.databaseOperations.processingTime}ms`);
            
          } catch (error) {
            const errorMessage = error instanceof Error ? error.message : String(error);
            console.log(`‚ùå Unified processing failed: ${errorMessage}`);
            if (error instanceof Error && error.stack) {
              console.log(`   Stack trace: ${error.stack}`);
            }
          }
        }
      } catch (llmError) {
        const errorMessage = llmError instanceof Error ? llmError.message : String(llmError);
        console.log(`‚ùå LLM processing failed: ${errorMessage}`);
        if (llmError instanceof Error && llmError.stack) {
          console.log(`   Stack trace: ${llmError.stack}`);
        }
      }
    }
    
    // Test 5: Database Verification
    console.log('\nüíæ STEP 5: Verifying Database State');
    console.log('----------------------------------');
    
    const entityCount = await entityRepo.count();
    const connectionCount = await connectionRepo.count();
    
    console.log(`‚úÖ Database verification:`);
    console.log(`   Total entities in database: ${entityCount}`);
    console.log(`   Total connections in database: ${connectionCount}`);
    
  } catch (error) {
    const errorObj = error instanceof Error ? error : new Error(String(error));
    logger.error('End-to-end test failed', errorObj);
    console.log(`‚ùå Test failed: ${errorObj.message}`);
    if (errorObj.stack) {
      console.log(`   Stack trace: ${errorObj.stack}`);
    }
  } finally {
    await app.close();
  }
  
  return results;
}

// Generate final report
function generateReport(results: TestResults): void {
  console.log('\nüìä END-TO-END TEST RESULTS SUMMARY');
  console.log('=====================================');
  
  const totalProcessingTime = 
    results.archiveProcessing.processingTime +
    results.llmProcessing.processingTime +
    results.databaseOperations.processingTime +
    results.redditApiTest.processingTime;
  
  console.log(`üïê Total Processing Time: ${totalProcessingTime}ms (${(totalProcessingTime/1000).toFixed(2)}s)`);
  console.log('');
  
  console.log('üìÅ Archive Processing:');
  console.log(`   ‚úÖ Files processed: ${results.archiveProcessing.filesProcessed}`);
  console.log(`   ‚úÖ Records extracted: ${results.archiveProcessing.recordsExtracted}`);
  console.log(`   ‚è±Ô∏è  Processing time: ${results.archiveProcessing.processingTime}ms`);
  console.log('');
  
  console.log('üåê Reddit API Collection:');
  console.log(`   ‚úÖ Posts retrieved: ${results.redditApiTest.postsRetrieved}`);
  console.log(`   ‚úÖ Comments retrieved: ${results.redditApiTest.commentsRetrieved}`);
  console.log(`   ‚è±Ô∏è  Processing time: ${results.redditApiTest.processingTime}ms`);
  console.log('');
  
  console.log('üß† LLM Processing:');
  console.log(`   ‚úÖ Content analyzed: ${results.llmProcessing.contentAnalyzed}`);
  console.log(`   ‚úÖ Entities extracted: ${results.llmProcessing.entitiesExtracted}`);
  console.log(`   ‚è±Ô∏è  Processing time: ${results.llmProcessing.processingTime}ms`);
  console.log('');
  
  console.log('üíæ Database Operations:');
  console.log(`   ‚úÖ Connections created: ${results.databaseOperations.connectionsCreated}`);
  console.log(`   ‚úÖ Mentions created: ${results.databaseOperations.mentionsCreated}`);
  console.log(`   ‚è±Ô∏è  Processing time: ${results.databaseOperations.processingTime}ms`);
  console.log('');
  
  // Success metrics
  const isSuccess = 
    results.archiveProcessing.recordsExtracted > 0 ||
    results.redditApiTest.postsRetrieved > 0;
  
  console.log(`üéØ Overall Test Status: ${isSuccess ? '‚úÖ SUCCESS' : '‚ùå FAILED'}`);
  console.log('');
  
  if (isSuccess) {
    console.log('üéâ END-TO-END REAL DATA FLOW VALIDATION COMPLETE!');
    console.log('   All core components are working with real data');
  } else {
    console.log('‚ö†Ô∏è  Some components need investigation');
    console.log('   Check individual step results above');
  }
}

// Run the test
if (require.main === module) {
  runEndToEndTest()
    .then(results => {
      generateReport(results);
      process.exit(0);
    })
    .catch(error => {
      console.error('‚ùå End-to-end test crashed:', error);
      process.exit(1);
    });
}

export { runEndToEndTest, TestResults };